{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 145 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#import movie ratings into RDD\n",
    "ratingLines = sc.textFile(\"C:/SparkCourse/ml-100k/u.data\")\n",
    "#import user details into RDD\n",
    "userLines = sc.textFile(\"///SparkCourse/ml-100k/u.user\")\n",
    "#import movie data into RDD\n",
    "movieLines = sc.textFile(\"C:/SparkCourse/ml-100k/u.item\")\n",
    "#import genre data into RDD\n",
    "genreLines = sc.textFile(\"C:/SparkCourse/ml-100k/u.genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratingRDD:\n",
      " [(943, 1330, 3), (943, 1228, 3), (943, 1188, 3), (943, 1074, 4), (943, 1067, 2)]\n",
      "occupationRDD:\n",
      " [(943, 'student'), (942, 'librarian'), (941, 'student')]\n",
      "ageRDD:\n",
      " [(943, '22'), (942, '48'), (941, '20')]\n",
      "movieRDD:\n",
      " [(1682, [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), (1681, [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), (1680, [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])]\n",
      "genreRDD:\n",
      " [(0, 'unknown'), (1, 'Action'), (2, 'Adventure')]\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#split on delimiter functions\n",
    "def splitRatingTab(line):\n",
    "    line = line.split('\\t')\n",
    "    return (int(line[0]), int(line[1]), int(line[2])) #(movieid, user, rating)\n",
    "def splitUserPipe(line):\n",
    "    line = line.split('|')\n",
    "    return (int(line[0]), line[3]) #(user, occupation)\n",
    "def splitMoviePipe(line):\n",
    "    line = line.split('|')\n",
    "    return (int(line[0]), list(listToIntElements(line[5:]))) #(movieid, genre_list[])\n",
    "def splitGenrePipe(line):\n",
    "    line = line.split('|')    \n",
    "    return (int(line[1]), line[0]) #(genreId, genre)\n",
    "\n",
    "def listToIntElements(lst):\n",
    "    \"\"\"conver the boolean text ('0', '1') genre value to integers (0, 1)\"\"\"\n",
    "    for cnt, _ in enumerate(lst):\n",
    "        lst[cnt] = int(_)\n",
    "    return lst\n",
    "\n",
    "# Transform to RDD as [(movieid, user, rating)] for movies, which are reviewed by viewers\n",
    "ratingRDD = ratingLines.map(lambda line: splitRatingTab(line))\n",
    "print ('ratingRDD:\\n',ratingRDD.top(5))\n",
    "\n",
    "# Transform to RDD as [(user, occupation)]\n",
    "occupationRDD = userLines.map(splitUserPipe)\n",
    "print ('occupationRDD:\\n',occupationRDD.top(3))\n",
    "\n",
    "# Transform to RDD as [(user, age)]\n",
    "ageRDD = userLines.map(lambda line: (int(line.split('|')[0]), line.split('|')[1]))\n",
    "print ('ageRDD:\\n',ageRDD.top(3))\n",
    "\n",
    "# Transform to RDD as [(movieid, genre_list)], genre is boolean value, movie can be in multiple genres\n",
    "movieRDD = movieLines.map(splitMoviePipe)\n",
    "print ('movieRDD:\\n',movieRDD.top(3))\n",
    "\n",
    "# Transform to RDD as [(genreId, genre)]\n",
    "genreRDD = genreLines.map(lambda line: splitGenrePipe(line))\n",
    "print ('genreRDD:\\n',genreRDD.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['unknown', 0], 1: ['Action', 1], 2: ['Adventure', 2], 3: ['Animation', 3], 4: [\"Children's\", 4], 5: ['Comedy', 5], 6: ['Crime', 6], 7: ['Documentary', 7], 8: ['Drama', 8], 9: ['Fantasy', 9], 10: ['Film-Noir', 10], 11: ['Horror', 11], 12: ['Musical', 12], 13: ['Mystery', 13], 14: ['Romance', 14], 15: ['Sci-Fi', 15], 16: ['Thriller', 16], 17: ['War', 17], 18: ['Western', 18]}\n"
     ]
    }
   ],
   "source": [
    "# Create a genre dictionary with {genreID: [genre, genreID]}\n",
    "with open('C:/SparkCourse/ml-100k/u.genre') as file:\n",
    "    genre = {}\n",
    "    for line in file:\n",
    "        #each line is of type [genere, genreid]\n",
    "        line = line.split('|')\n",
    "        #convert genreid to int, to remove new line '\\n' at the end of string\n",
    "        genre[int(line[1])] = [line[0], int(line[1])]\n",
    "    print (genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, ((264, 3), [0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (4, ((303, 5), [0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]\n",
      "Wall time: 8.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#join Transformed rating RDD [(movieid, (user, rating))] and movieRDD [(movieid, genre] to get all genres; \n",
    "#then Transform to [(movieid,((userid, rating), genre) )]\n",
    "joinRatingMovieGenres = ratingRDD.map(lambda line: (line[0], (line[1], line[2]))).join(movieRDD)\n",
    "print (joinRatingMovieGenres.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('librarian', (1, [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), ('librarian', (1, [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]))]\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Transform joinRatingMovieGenres to RDD [userid, (rating, genre)] and join with occupationRDD [(userid, occupation)]\n",
    "#to Transform to [(occupation, ((1_ratingCount, genre)))]\n",
    "transRatingMovieGenres = joinRatingMovieGenres.map(lambda line: (line[1][0][0], (line[1][0][1], line[1][1])))\n",
    "joinRatingGenresOccup = transRatingMovieGenres.join(occupationRDD).map(lambda line: (line[1][1], (1, line[1][0][1]))).cache()\n",
    "print (joinRatingGenresOccup.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('executive', (4382, [12, 920])), ('retired', (1315, [1, 291]))]\n",
      "Wall time: 7.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Transform by Aggregating the ratingCount and genreCount to [(occupation, (totalRatings, [cntGenresRating]))]\n",
    "totalRatingGenreCntByOccupation = joinRatingGenresOccup.reduceByKey(lambda x, y: ((x[0]+y[0]), [(x[1][0]+y[1][0]), (x[1][1]+y[1][1])]))\n",
    "print (totalRatingGenreCntByOccupation.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('executive', (4382, [12, 920, 496, 122, 404, 1284, 319, 86, 1657, 70, 87, 349, 141, 261, 769, 438, 755, 199, 78])), ('retired', (1315, [1, 291, 155, 29, 107, 413, 90, 20, 500, 21, 32, 110, 38, 83, 226, 134, 233, 63, 26]))]\n"
     ]
    }
   ],
   "source": [
    "#Transform by Aggregating the ratingCount and genreCount to [(occupation, (totalRatings, [cntGenresRating]))]\n",
    "totalRatingGenreCntByOccupation = joinRatingGenresOccup.reduceByKey(lambda x, y: ((x[0]+y[0]), [(x[1][0]+y[1][0]), (x[1][1]+y[1][1]), (x[1][2]+y[1][2]), (x[1][3]+y[1][3]), (x[1][4]+y[1][4]), (x[1][5]+y[1][5]), (x[1][6]+y[1][6]), (x[1][7]+y[1][7]), (x[1][8]+y[1][8]), (x[1][9]+y[1][9]), (x[1][10]+y[1][10]), (x[1][11]+y[1][11]), (x[1][12]+y[1][12]), (x[1][13]+y[1][13]), (x[1][14]+y[1][14]), (x[1][15]+y[1][15]), (x[1][16]+y[1][16]), (x[1][17]+y[1][17]), (x[1][18]+y[1][18])]))\n",
    "print (totalRatingGenreCntByOccupation.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#using the genre dictionary defined outside to assign the genre values to every count\n",
    "def calcPercent(line):\n",
    "    '''calculate the percentage of each rating for the input (totalRatings, [%GenreRatings])\n",
    "    update the genre dictionary from {genreId:('unknown', genreId)} to {genreId:('unknown', [prcntRating)}'''\n",
    "    tot = line[0]\n",
    "    lst = []\n",
    "    for i, val in enumerate(line[1]):\n",
    "        calc = round(float((val*100)/tot))\n",
    "        lst.append(calc)\n",
    "        genre[i][1] = calc\n",
    "    return (tot, list(genre.values()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('executive', (4382, [['unknown', 0], ['Action', 21], ['Adventure', 11], ['Animation', 3], [\"Children's\", 9], ['Comedy', 29], ['Crime', 7], ['Documentary', 2], ['Drama', 38], ['Fantasy', 2], ['Film-Noir', 2], ['Horror', 8], ['Musical', 3], ['Mystery', 6], ['Romance', 18], ['Sci-Fi', 10], ['Thriller', 17], ['War', 5], ['Western', 2]])), ('retired', (1315, [['unknown', 0], ['Action', 22], ['Adventure', 12], ['Animation', 2], [\"Children's\", 8], ['Comedy', 31], ['Crime', 7], ['Documentary', 2], ['Drama', 38], ['Fantasy', 2], ['Film-Noir', 2], ['Horror', 8], ['Musical', 3], ['Mystery', 6], ['Romance', 17], ['Sci-Fi', 10], ['Thriller', 18], ['War', 5], ['Western', 2]]))]\n"
     ]
    }
   ],
   "source": [
    "#Transform totalRatingGenreCntByOccupation to [occupation, (totalRatings, [genre, %GenreRatings])]\n",
    "prcntGenreRatingsByOccupation = totalRatingGenreCntByOccupation.map(lambda line: (line[0], calcPercent(line[1])))\n",
    "print(prcntGenreRatingsByOccupation.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Occupation : unk | Act | Adv | Ani | Chi | Com | Cri | Doc | Dra | Fan | Fil | Hor | Mus | Mys | Rom | Sci | Thr | War | Wes\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "    executive :   0 |  21 |  11 |   3 |   9 |  29 |   7 |   2 |  38 |   2 |   2 |   8 |   3 |   6 |  18 |  10 |  17 |   5 |   2\n",
      "      retired :   0 |  20 |  12 |   3 |   9 |  30 |   7 |   2 |  40 |   2 |   2 |   8 |   3 |   6 |  17 |  10 |  16 |   5 |   2\n",
      "     educator :   0 |  20 |  12 |   3 |   9 |  30 |   7 |   2 |  40 |   2 |   2 |   8 |   3 |   6 |  17 |  10 |  16 |   5 |   2\n",
      "administrator :   0 |  21 |  12 |   3 |   9 |  29 |   7 |   2 |  39 |   2 |   2 |   8 |   3 |   6 |  18 |  10 |  16 |   5 |   2\n",
      "    homemaker :   0 |  21 |  12 |   3 |   9 |  29 |   7 |   2 |  39 |   2 |   2 |   8 |   3 |   6 |  18 |  10 |  16 |   5 |   2\n",
      "   programmer :   0 |  21 |  12 |   3 |   9 |  29 |   7 |   2 |  39 |   2 |   2 |   8 |   3 |   6 |  18 |  10 |  16 |   5 |   2\n",
      "    scientist :   0 |  21 |  12 |   3 |   9 |  28 |   7 |   2 |  39 |   1 |   2 |   8 |   3 |   7 |  18 |  10 |  16 |   5 |   2\n",
      "   healthcare :   0 |  21 |  12 |   3 |  10 |  29 |   7 |   2 |  39 |   2 |   2 |   8 |   3 |   6 |  17 |  10 |  16 |   5 |   2\n",
      "       writer :   0 |  21 |  12 |   3 |  10 |  29 |   7 |   2 |  39 |   2 |   2 |   8 |   3 |   6 |  17 |  10 |  16 |   5 |   2\n",
      "   technician :   0 |  22 |  12 |   2 |   8 |  31 |   8 |   2 |  38 |   1 |   3 |   8 |   3 |   6 |  18 |  10 |  16 |   5 |   2\n",
      "        other :   0 |  21 |  12 |   3 |  10 |  29 |   7 |   2 |  39 |   2 |   2 |   8 |   3 |   6 |  18 |  10 |  16 |   5 |   2\n",
      "       artist :   0 |  21 |  12 |   3 |  10 |  29 |   7 |   2 |  39 |   2 |   2 |   9 |   3 |   6 |  17 |  10 |  16 |   5 |   2\n",
      "      student :   0 |  21 |  12 |   3 |  10 |  29 |   7 |   2 |  39 |   2 |   2 |   9 |   3 |   6 |  17 |  10 |  16 |   5 |   2\n",
      "entertainment :   0 |  20 |  12 |   3 |  10 |  28 |   7 |   2 |  40 |   1 |   2 |   8 |   3 |   6 |  17 |   9 |  16 |   3 |   2\n",
      "     salesman :   0 |  21 |  14 |   3 |  10 |  29 |   8 |   2 |  38 |   1 |   2 |   9 |   3 |   6 |  17 |  11 |  16 |   5 |   2\n",
      "     engineer :   0 |  20 |  12 |   3 |  10 |  30 |   7 |   2 |  39 |   2 |   2 |   8 |   3 |   6 |  17 |  10 |  16 |   5 |   2\n",
      "         none :   0 |  21 |  13 |   2 |   9 |  29 |   6 |   2 |  40 |   1 |   3 |   8 |   3 |   5 |  17 |  10 |  15 |   4 |   2\n",
      "       doctor :   0 |  21 |  13 |   2 |   9 |  29 |   6 |   2 |  40 |   1 |   3 |   8 |   3 |   5 |  17 |  10 |  15 |   4 |   2\n",
      "    librarian :   0 |  20 |  12 |   3 |   9 |  29 |   7 |   2 |  38 |   2 |   2 |   9 |   3 |   6 |  18 |   9 |  16 |   5 |   2\n",
      "       lawyer :   0 |  20 |  12 |   3 |   9 |  29 |   7 |   2 |  39 |   2 |   2 |   8 |   3 |   6 |  17 |  10 |  15 |   5 |   2\n",
      "    marketing :   0 |  20 |  12 |   3 |   9 |  29 |   7 |   2 |  39 |   2 |   2 |   8 |   3 |   6 |  17 |  10 |  15 |   5 |   2\n"
     ]
    }
   ],
   "source": [
    "#Collect the RDD data into python object [(occupation, (tot, [[genre, genrePercent], [], ...]))] and print:\n",
    "resultList = prcntGenreRatingsByOccupation.collect()\n",
    "\n",
    "#print the header\n",
    "print ('Occupation'.rjust(13),':', genre[0][0][:3],'|', genre[1][0][:3],'|', genre[2][0][:3],'|', genre[3][0][:3],'|', genre[4][0][:3],'|', genre[5][0][:3],'|', genre[6][0][:3],'|', genre[7][0][:3],'|', genre[8][0][:3],'|', genre[9][0][:3],'|', genre[10][0][:3],'|', genre[11][0][:3],'|', genre[12][0][:3],'|', genre[13][0][:3],'|', genre[14][0][:3],'|', genre[15][0][:3],'|', genre[16][0][:3],'|', genre[17][0][:3],'|', genre[18][0][:3])\n",
    "print ('-'*127)\n",
    "\n",
    "#print occupation: genreRatingPercentages in 2 digits\n",
    "for i in resultList:\n",
    "    print (i[0].rjust(13),':', str(i[1][1][0][1]).rjust(3),'|', str(i[1][1][1][1]).rjust(3),'|', str(i[1][1][2][1]).rjust(3),'|', str(i[1][1][3][1]).rjust(3),'|', str(i[1][1][4][1]).rjust(3),'|', str(i[1][1][5][1]).rjust(3),'|', str(i[1][1][6][1]).rjust(3),'|', str(i[1][1][7][1]).rjust(3),'|', str(i[1][1][8][1]).rjust(3),'|', str(i[1][1][9][1]).rjust(3),'|', str(i[1][1][10][1]).rjust(3),'|', str(i[1][1][11][1]).rjust(3),'|', str(i[1][1][12][1]).rjust(3),'|', str(i[1][1][13][1]).rjust(3),'|', str(i[1][1][14][1]).rjust(3),'|', str(i[1][1][15][1]).rjust(3),'|', str(i[1][1][16][1]).rjust(3),'|', str(i[1][1][17][1]).rjust(3),'|', str(i[1][1][18][1]).rjust(3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Testing the reduceByKey() Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('librarian', (1, [0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), ('librarian', (1, [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('librarian', (2, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0))]\n"
     ]
    }
   ],
   "source": [
    "result = rdd.reduceByKey(lambda x, y: ((x[0]+y[0]), (x[1][0]+y[1][0]), (x[1][1]+y[1][1]), (x[1][2]+y[1][2]), (x[1][3]+y[1][3]), (x[1][4]+y[1][4]), (x[1][5]+y[1][5]), (x[1][6]+y[1][6]), (x[1][7]+y[1][7]), (x[1][8]+y[1][8]), (x[1][9]+y[1][9]), (x[1][10]+y[1][10]), (x[1][11]+y[1][11])))\n",
    "print (result.top(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('librarian', (2, 0, 0, 2))]\n"
     ]
    }
   ],
   "source": [
    "# Works because only 2 elements\n",
    "rdd1 = sc.parallelize([('librarian', (1, [0, 0, 1, 0])), ('librarian', (1, [0, 1, 0, 0]))])\n",
    "result = rdd1.reduceByKey(lambda x, y: ((x[0]+y[0]),(x[1][0]+y[1][0]), (x[1][1]+y[1][1]), (x[1][2]+y[1][2])))\n",
    "print (result.top(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 39.0 failed 1 times, most recent failure: Lost task 1.0 in stage 39.0 (TID 68, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 362, in func\n    return f(iterator)\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 1865, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 272, in mergeCombiners\n    d[k] = comb(d[k], v) if k in d else v\n  File \"<ipython-input-16-56c286a9920e>\", line 3, in <lambda>\nTypeError: 'int' object is not subscriptable\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 362, in func\n    return f(iterator)\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 1865, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 272, in mergeCombiners\n    d[k] = comb(d[k], v) if k in d else v\n  File \"<ipython-input-16-56c286a9920e>\", line 3, in <lambda>\nTypeError: 'int' object is not subscriptable\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-56c286a9920e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtop\u001b[1;34m(self, num, key)\u001b[0m\n\u001b[0;32m   1284\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mheapq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1286\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopIterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtakeOrdered\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    848\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 850\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    851\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    822\u001b[0m         \"\"\"\n\u001b[0;32m    823\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 824\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    825\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1160\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    319\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 39.0 failed 1 times, most recent failure: Lost task 1.0 in stage 39.0 (TID 68, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 362, in func\n    return f(iterator)\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 1865, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 272, in mergeCombiners\n    d[k] = comb(d[k], v) if k in d else v\n  File \"<ipython-input-16-56c286a9920e>\", line 3, in <lambda>\nTypeError: 'int' object is not subscriptable\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 362, in func\n    return f(iterator)\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 1865, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 272, in mergeCombiners\n    d[k] = comb(d[k], v) if k in d else v\n  File \"<ipython-input-16-56c286a9920e>\", line 3, in <lambda>\nTypeError: 'int' object is not subscriptable\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# Fails : When you reduceByKey you have to return the same structure you have received, otherwise the next time you will meet \n",
    "# a value of the same key and will try to reduce it your function will not work\n",
    "rdd2 = sc.parallelize([('librarian', (1, [0, 0, 1, 0])), ('librarian', (1, [0, 1, 0, 0])), ('librarian', (1, [0, 1, 0, 0]))])\n",
    "result = rdd2.reduceByKey(lambda x, y: ((x[0]+y[0]),(x[1][0]+y[1][0]), (x[1][1]+y[1][1]), (x[1][2]+y[1][2]), (x[1][3]+y[1][3]) ))\n",
    "print (result.top(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('librarian', (3, [0, 2, 1, 0]))]\n"
     ]
    }
   ],
   "source": [
    "# Success : As reduceByKey returns the same structure you have received\n",
    "rdd3 = sc.parallelize([('librarian', (1, [0, 0, 1, 0])), ('librarian', (1, [0, 1, 0, 0])), ('librarian', (1, [0, 1, 0, 0]))])\n",
    "result = rdd3.reduceByKey(lambda x, y: ((x[0]+y[0]),[(x[1][0]+y[1][0]), (x[1][1]+y[1][1]), (x[1][2]+y[1][2]), (x[1][3]+y[1][3])]))\n",
    "print (result.top(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Python Spark program for Analysis of Movie Ratings percentages across Occupation and Movie Genre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Occupation : unk | Act | Adv | Ani | Chi | Com | Cri | Doc | Dra | Fan | Fil | Hor | Mus | Mys | Rom | Sci | Thr | War | Wes\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "    executive :   0 |  21 |  11 |   3 |   9 |  29 |   7 |   2 |  38 |   2 |   2 |   8 |   3 |   6 |  18 |  10 |  17 |   5 |   2\n",
      "      retired :   0 |  20 |  12 |   3 |   9 |  30 |   7 |   2 |  40 |   2 |   2 |   8 |   3 |   6 |  17 |  10 |  16 |   5 |   2\n",
      "     educator :   0 |  20 |  12 |   3 |   9 |  30 |   7 |   2 |  40 |   2 |   2 |   8 |   3 |   6 |  17 |  10 |  16 |   5 |   2\n",
      "administrator :   0 |  21 |  12 |   3 |   9 |  29 |   7 |   2 |  39 |   2 |   2 |   8 |   3 |   6 |  18 |  10 |  16 |   5 |   2\n",
      "    homemaker :   0 |  21 |  12 |   3 |   9 |  29 |   7 |   2 |  39 |   2 |   2 |   8 |   3 |   6 |  18 |  10 |  16 |   5 |   2\n",
      "   programmer :   0 |  21 |  12 |   3 |   9 |  29 |   7 |   2 |  39 |   2 |   2 |   8 |   3 |   6 |  18 |  10 |  16 |   5 |   2\n",
      "    scientist :   0 |  21 |  12 |   3 |   9 |  28 |   7 |   2 |  39 |   1 |   2 |   8 |   3 |   7 |  18 |  10 |  16 |   5 |   2\n",
      "   healthcare :   0 |  21 |  12 |   3 |  10 |  29 |   7 |   2 |  39 |   2 |   2 |   8 |   3 |   6 |  17 |  10 |  16 |   5 |   2\n",
      "       writer :   0 |  21 |  12 |   3 |  10 |  29 |   7 |   2 |  39 |   2 |   2 |   8 |   3 |   6 |  17 |  10 |  16 |   5 |   2\n",
      "   technician :   0 |  22 |  12 |   2 |   8 |  31 |   8 |   2 |  38 |   1 |   3 |   8 |   3 |   6 |  18 |  10 |  16 |   5 |   2\n",
      "        other :   0 |  21 |  12 |   3 |  10 |  29 |   7 |   2 |  39 |   2 |   2 |   8 |   3 |   6 |  18 |  10 |  16 |   5 |   2\n",
      "       artist :   0 |  21 |  12 |   3 |  10 |  29 |   7 |   2 |  39 |   2 |   2 |   9 |   3 |   6 |  17 |  10 |  16 |   5 |   2\n",
      "      student :   0 |  21 |  12 |   3 |  10 |  29 |   7 |   2 |  39 |   2 |   2 |   9 |   3 |   6 |  17 |  10 |  16 |   5 |   2\n",
      "entertainment :   0 |  20 |  12 |   3 |  10 |  28 |   7 |   2 |  40 |   1 |   2 |   8 |   3 |   6 |  17 |   9 |  16 |   3 |   2\n",
      "     salesman :   0 |  21 |  14 |   3 |  10 |  29 |   8 |   2 |  38 |   1 |   2 |   9 |   3 |   6 |  17 |  11 |  16 |   5 |   2\n",
      "     engineer :   0 |  20 |  12 |   3 |  10 |  30 |   7 |   2 |  39 |   2 |   2 |   8 |   3 |   6 |  17 |  10 |  16 |   5 |   2\n",
      "         none :   0 |  21 |  13 |   2 |   9 |  29 |   6 |   2 |  40 |   1 |   3 |   8 |   3 |   5 |  17 |  10 |  15 |   4 |   2\n",
      "       doctor :   0 |  21 |  13 |   2 |   9 |  29 |   6 |   2 |  40 |   1 |   3 |   8 |   3 |   5 |  17 |  10 |  15 |   4 |   2\n",
      "    librarian :   0 |  20 |  12 |   3 |   9 |  29 |   7 |   2 |  38 |   2 |   2 |   9 |   3 |   6 |  18 |   9 |  16 |   5 |   2\n",
      "       lawyer :   0 |  20 |  12 |   3 |   9 |  29 |   7 |   2 |  39 |   2 |   2 |   8 |   3 |   6 |  17 |  10 |  15 |   5 |   2\n",
      "    marketing :   0 |  20 |  12 |   3 |   9 |  29 |   7 |   2 |  39 |   2 |   2 |   8 |   3 |   6 |  17 |  10 |  15 |   5 |   2\n",
      "Wall time: 39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "##############################################################################\n",
    "### Analysis of Movie Ratings percentages across Occupation and Movie Genre\n",
    "##############################################################################\n",
    "\n",
    "####from pyspark import SparkConf, SparkContext\n",
    "\n",
    "####conf = SparkConf().SetMaster(\"local\").setAppName(\"MovieCountByRatings\")\n",
    "####sc = SparkContext(conf = conf)\n",
    "\n",
    "#import movie ratings into RDD\n",
    "ratingLines = sc.textFile(\"C:/SparkCourse/ml-100k/u.data\")\n",
    "#import user details into RDD\n",
    "userLines = sc.textFile(\"///SparkCourse/ml-100k/u.user\")\n",
    "#import movie data into RDD\n",
    "movieLines = sc.textFile(\"C:/SparkCourse/ml-100k/u.item\")\n",
    "#import genre data into RDD\n",
    "genreLines = sc.textFile(\"C:/SparkCourse/ml-100k/u.genre\")\n",
    "\n",
    "#split on delimiter functions\n",
    "def splitRatingTab(line):\n",
    "    line = line.split('\\t')\n",
    "    return (int(line[0]), int(line[1]), int(line[2])) #(movieid, user, rating)\n",
    "def splitUserPipe(line):\n",
    "    line = line.split('|')\n",
    "    return (int(line[0]), line[3]) #(user, occupation)\n",
    "def splitMoviePipe(line):\n",
    "    line = line.split('|')\n",
    "    return (int(line[0]), list(listToIntElements(line[5:]))) #(movieid, genre_list[])\n",
    "\n",
    "def listToIntElements(lst):\n",
    "    \"\"\"conver the boolean text ('0', '1') genre value to integers (0, 1)\"\"\"\n",
    "    for cnt, _ in enumerate(lst):\n",
    "        lst[cnt] = int(_)\n",
    "    return lst\n",
    "\n",
    "\n",
    "\n",
    "# Transform to RDD as [(movieid, user, rating)] for movies, which are reviewed by viewers\n",
    "ratingRDD = ratingLines.map(lambda line: splitRatingTab(line))\n",
    "#print ('ratingRDD:\\n',ratingRDD.top(5))\n",
    "\n",
    "# Transform to RDD as [(user, occupation)]\n",
    "occupationRDD = userLines.map(splitUserPipe)\n",
    "#print ('occupationRDD:\\n',occupationRDD.top(3))\n",
    "\n",
    "# Transform to RDD as [(user, age)]\n",
    "ageRDD = userLines.map(lambda line: (int(line.split('|')[0]), line.split('|')[1]))\n",
    "#print ('ageRDD:\\n',ageRDD.top(3))\n",
    "\n",
    "# Transform to RDD as [(movieid, genre_list)], genre is boolean value, movie can be in multiple genres\n",
    "movieRDD = movieLines.map(splitMoviePipe)\n",
    "#print ('movieRDD:\\n',movieRDD.top(3))\n",
    "\n",
    "\n",
    "# Create a genre dictionary with {genreID: [genre, genreID]}\n",
    "with open('C:/SparkCourse/ml-100k/u.genre') as file:\n",
    "    genre = {}\n",
    "    for line in file:\n",
    "        #each line is of type [genere, genreid]\n",
    "        line = line.split('|')\n",
    "        #convert genreid to int, to remove new line '\\n' at the end of string\n",
    "        genre[int(line[1])] = [line[0], int(line[1])]\n",
    "\n",
    "        \n",
    "#using the genre dictionary defined outside to assign the genre values to every count\n",
    "def calcPercent(line):\n",
    "    '''calculate the percentage of each rating for the input (totalRatings, [%GenreRatings])\n",
    "    update the genre dictionary from {genreId:('unknown', genreId)} to {genreId:('unknown', [prcntRating)}'''\n",
    "    tot = line[0]\n",
    "    lst = []\n",
    "    for i, val in enumerate(line[1]):\n",
    "        calc = round(float((val*100)/tot))\n",
    "        lst.append(calc)\n",
    "        genre[i][1] = calc\n",
    "    return (tot, list(genre.values()))\n",
    "\n",
    "\n",
    "#join Transformed rating RDD [(movieid, (user, rating))] and movieRDD [(movieid, genre] to get all genres; \n",
    "#then Transform to [(movieid,((userid, rating), genre) )]\n",
    "joinRatingMovieGenres = ratingRDD.map(lambda line: (line[0], (line[1], line[2]))).join(movieRDD)\n",
    "\n",
    "#Transform joinRatingMovieGenres to RDD [userid, (rating, genre)] and join with occupationRDD [(userid, occupation)]\n",
    "#to Transform to [(occupation, ((1_ratingCount, genre)))]\n",
    "transRatingMovieGenres = joinRatingMovieGenres.map(lambda line: (line[1][0][0], (line[1][0][1], line[1][1])))\n",
    "joinRatingGenresOccup = transRatingMovieGenres.join(occupationRDD).map(lambda line: (line[1][1], (1, line[1][0][1]))).cache()\n",
    "\n",
    "#Transform by Aggregating the ratingCount and genreCount to [(occupation, (totalRatings, [cntGenresRating]))]\n",
    "totalRatingGenreCntByOccupation = joinRatingGenresOccup.reduceByKey(lambda x, y: ((x[0]+y[0]), [(x[1][0]+y[1][0]), (x[1][1]+y[1][1]), (x[1][2]+y[1][2]), (x[1][3]+y[1][3]), (x[1][4]+y[1][4]), (x[1][5]+y[1][5]), (x[1][6]+y[1][6]), (x[1][7]+y[1][7]), (x[1][8]+y[1][8]), (x[1][9]+y[1][9]), (x[1][10]+y[1][10]), (x[1][11]+y[1][11]), (x[1][12]+y[1][12]), (x[1][13]+y[1][13]), (x[1][14]+y[1][14]), (x[1][15]+y[1][15]), (x[1][16]+y[1][16]), (x[1][17]+y[1][17]), (x[1][18]+y[1][18])]))\n",
    "\n",
    "#Transform totalRatingGenreCntByOccupation to [occupation, (totalRatings, [genre, %GenreRatings])]\n",
    "prcntGenreRatingsByOccupation = totalRatingGenreCntByOccupation.map(lambda line: (line[0], calcPercent(line[1])))\n",
    "\n",
    "\n",
    "#Collect the RDD data into python object [(occupation, (tot, [[genre, genrePercent], [], ...]))] and print:\n",
    "resultList = prcntGenreRatingsByOccupation.collect()\n",
    "\n",
    "#print the header\n",
    "print ('Occupation'.rjust(13),':', genre[0][0][:3],'|', genre[1][0][:3],'|', genre[2][0][:3],'|', genre[3][0][:3],'|', genre[4][0][:3],'|', genre[5][0][:3],'|', genre[6][0][:3],'|', genre[7][0][:3],'|', genre[8][0][:3],'|', genre[9][0][:3],'|', genre[10][0][:3],'|', genre[11][0][:3],'|', genre[12][0][:3],'|', genre[13][0][:3],'|', genre[14][0][:3],'|', genre[15][0][:3],'|', genre[16][0][:3],'|', genre[17][0][:3],'|', genre[18][0][:3])\n",
    "print ('-'*127)\n",
    "\n",
    "#print occupation: genreRatingPercentages in 2 digits\n",
    "for i in resultList:\n",
    "    print (i[0].rjust(13),':', str(i[1][1][0][1]).rjust(3),'|', str(i[1][1][1][1]).rjust(3),'|', str(i[1][1][2][1]).rjust(3),'|', str(i[1][1][3][1]).rjust(3),'|', str(i[1][1][4][1]).rjust(3),'|', str(i[1][1][5][1]).rjust(3),'|', str(i[1][1][6][1]).rjust(3),'|', str(i[1][1][7][1]).rjust(3),'|', str(i[1][1][8][1]).rjust(3),'|', str(i[1][1][9][1]).rjust(3),'|', str(i[1][1][10][1]).rjust(3),'|', str(i[1][1][11][1]).rjust(3),'|', str(i[1][1][12][1]).rjust(3),'|', str(i[1][1][13][1]).rjust(3),'|', str(i[1][1][14][1]).rjust(3),'|', str(i[1][1][15][1]).rjust(3),'|', str(i[1][1][16][1]).rjust(3),'|', str(i[1][1][17][1]).rjust(3),'|', str(i[1][1][18][1]).rjust(3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
